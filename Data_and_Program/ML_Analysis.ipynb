{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL PROJECT - JEN, THEA, SARATH \n",
    "### --- DATING ANALYSIS ---\n",
    "### PREDICT WHETHER DATERS WANT TO MEET AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as pltt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "scorer = make_scorer(fbeta_score, beta=1.5)\n",
    "\n",
    "# Some warnings tend to pop up during grid search\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8378 observations and 195 features\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Data/Speed_Dating_Data.csv', encoding='latin-1', engine=\"python\")\n",
    "\n",
    "print(data.shape[0], 'observations and', data.shape[1], 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    174\n",
       "int64       13\n",
       "object       8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What kind of data are we dealing with?\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    277\n",
       "0.0    274\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Men (1) and women (0)\n",
    "data.groupby('iid').mean().gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.835283\n",
       "1    0.164717\n",
       "Name: match, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage of pairs resulted in a match?\n",
    "data['match'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iid   id  gender  idg  condtn  wave  round  position  positin1  order  ...  \\\n",
       "0    1  1.0       0    1       1     1     10         7       NaN      4  ...   \n",
       "1    1  1.0       0    1       1     1     10         7       NaN      3  ...   \n",
       "2    1  1.0       0    1       1     1     10         7       NaN     10  ...   \n",
       "3    1  1.0       0    1       1     1     10         7       NaN      5  ...   \n",
       "4    1  1.0       0    1       1     1     10         7       NaN      7  ...   \n",
       "\n",
       "   attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  intel5_3  \\\n",
       "0      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "1      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "2      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "3      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "4      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "\n",
       "   fun5_3  amb5_3  \n",
       "0     NaN     NaN  \n",
       "1     NaN     NaN  \n",
       "2     NaN     NaN  \n",
       "3     NaN     NaN  \n",
       "4     NaN     NaN  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waves 6-11 rated importance, others allocated 100 points\n",
    "points = (data.wave != 6 ) &\\\n",
    "    (data.wave != 7 ) &\\\n",
    "    (data.wave != 8 ) &\\\n",
    "    (data.wave != 9 ) &\\\n",
    "    (data.wave != 10 ) &\\\n",
    "    (data.wave != 11 )\n",
    "\n",
    "data = data[points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['match', 'dec_o', 'dec', 'like_o', 'like', 'fun_o', 'fun', 'shar_o',\n",
       "       'shar', 'attr_o', 'attr', 'prob_o', 'prob'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_corrs = data.select_dtypes(include=[np.number])\\\n",
    ".corrwith(data.match)\\\n",
    ".sort_values(ascending=False)\n",
    "\n",
    "match_corrs = match_corrs[match_corrs > .25].index\n",
    "\n",
    "data = data.dropna(subset=['id', 'pid'], axis=0)\n",
    "\n",
    "for i in match_corrs[1:]:\n",
    "    del data[i]\n",
    "\n",
    "# Other columns that are too predictive\n",
    "#del data['int_corr']\n",
    "del data['them_cal']\n",
    "del data['you_call']\n",
    "\n",
    "del data['field'] # redundant\n",
    "\n",
    "match_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_numeric(col, df=data, median=False):\n",
    "    try:\n",
    "        df[col] = df[col].str.replace(',', '')\n",
    "    except:\n",
    "        print('Column is not a string!')\n",
    "    \n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "    if median:\n",
    "        df[col] = df[col].fillna(data.fillna.median())\n",
    "    \n",
    "    else:\n",
    "        df[col] = df[col].fillna(-100)\n",
    "    \n",
    "    return df\n",
    "\n",
    "for i in 'zipcode mn_sat tuition income'.split():\n",
    "    data = fix_numeric(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undergra, from, career, "
     ]
    }
   ],
   "source": [
    "for i in data.columns:\n",
    "    if data[i].dtype == \"O\":\n",
    "        data[i] = data[i].str.lower()\n",
    "        print(i, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Benchmark: 0.39567670124395443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "data_benchmark = pd.get_dummies(data)\n",
    "data_benchmark.fillna(data_benchmark.median(), inplace=True)\n",
    "\n",
    "xbench = data_benchmark.drop('match', axis=1)\n",
    "ybench = data_benchmark['match']\n",
    "\n",
    "dum = DummyClassifier(strategy='constant', constant=1)\n",
    "\n",
    "cvs = cross_val_score(dum, xbench, ybench, scoring=scorer, cv=5)\n",
    "\n",
    "benchmark_score = cvs.mean()\n",
    "\n",
    "print('Naive Benchmark:', benchmark_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['from_new_england'] = 0\n",
    "for i in 'massachu connect rhode vermont vt hampsh maine boston cambridge'.split():\n",
    "    data['from_new_england'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                                   (data['undergra'].str.contains(i)), 1,\n",
    "                                        data['from_new_england'])\n",
    "    \n",
    "data['from_china'] = 0\n",
    "for i in 'china beijing shanghai hong taiwan'.split():\n",
    "    data['from_china'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                                  (data['undergra'].str.contains(i)), 1,\n",
    "                                  data['from_china'])\n",
    "\n",
    "data['from_india'] = 0\n",
    "for i in 'india delhi bangalore'.split():\n",
    "    data['from_india'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                                  (data['undergra'].str.contains(i)), 1,\n",
    "                                  data['from_india'])\n",
    "    \n",
    "data['from_europe'] = 0\n",
    "for i in 'europe germany italy france spain poland portugal netherlands holland sweden switz greece belgium paris rome'.split():\n",
    "    data['from_europe'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                                   (data['undergra'].str.contains(i)), 1,\n",
    "                                   data['from_europe'])\n",
    "\n",
    "data['from_uk'] = 0\n",
    "for i in 'london england uk britain scotland ireland kingdom oxford'.split():\n",
    "    data['from_uk'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                                   (data['undergra'].str.contains(i)), 1,\n",
    "                               data['from_uk'])\n",
    "\n",
    "data['from_ny'] = 0\n",
    "for i in ['new york', 'ny']:\n",
    "    data['from_ny'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                               (data['undergra'].str.contains(i)), 1,\n",
    "                               data['from_ny'])\n",
    "\n",
    "data['from_nj'] = 0\n",
    "for i in ['new jersey', 'nj']:\n",
    "    data['from_nj'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                               (data['undergra'].str.contains(i)), 1,\n",
    "                               data['from_nj'])\n",
    "\n",
    "data['from_california'] = 0\n",
    "for i in 'cali diego francisco jose davis sacramento oakland clara angeles ucla stanford berkeley alto torrance'.split():\n",
    "    data['from_california'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                                   (data['undergra'].str.contains(i)), 1,\n",
    "                                       data['from_california'])\n",
    "    \n",
    "data['from_texas'] = 0\n",
    "for i in 'texas tx dallas austin houston dfw antonio'.split():\n",
    "    data['from_texas'] = np.where((data['from'].str.contains(i)) |\\\n",
    "                                   (data['undergra'].str.contains(i)), 1,\n",
    "                                  data['from_texas'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['career_education'] = 0\n",
    "for i in 'professor teach academ educ'.split():\n",
    "    data['career_education'] = np.where(data['career'].str.contains(i),\n",
    "                                        1, data['career_education'])\n",
    "\n",
    "data['career_law'] = 0\n",
    "for i in 'law attorney legal defender counsel'.split():\n",
    "    data['career_law'] = np.where(data['career'].str.contains(i),\n",
    "                                  1, data['career_law'])\n",
    "\n",
    "data['career_medicine'] = 0\n",
    "for i in 'doctor dr physician md medical m.d. cardio dentist surg'.split():\n",
    "    data['career_medicine'] = np.where(data['career'].str.contains(i),\n",
    "                                       1, data['career_medicine'])\n",
    "\n",
    "data['career_business'] = 0\n",
    "for i in 'business mba m.b.a. consult manage ceo c.e.o. entre finance venture market strategy invest bank equity'.split():\n",
    "    data['career_business'] = np.where(data['career'].str.contains(i),\n",
    "                                       1, data['career_business'])\n",
    "    \n",
    "data['career_science'] = 0\n",
    "for i in 'sci research biolo chemi'.split():\n",
    "    data['career_science'] = np.where(data['career'].str.contains(i),\n",
    "                                      1, data['career_science'])\n",
    "\n",
    "data['career_gov'] = 0\n",
    "for i in 'gov diplo poli'.split():\n",
    "    data['career_gov'] = np.where(data['career'].str.contains(i),\n",
    "                                  1, data['career_gov'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['school_columbia'] = 0\n",
    "for i in 'columbia'.split():\n",
    "    data['school_columbia'] = np.where(data['undergra'].str.contains(i),\n",
    "                                       1, data['school_columbia'])\n",
    "\n",
    "# Ivy league and other prestigious schools\n",
    "data['school_ivy'] = 0\n",
    "for i in 'dartmouth cornell princeton penn yale brown harvard stanford mit berkeley oxford'.split():\n",
    "    data['school_ivy'] = np.where(data['undergra'].str.contains(i),\n",
    "                                  1, data['school_ivy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5761, 821)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummify\n",
    "data = pd.get_dummies(data, prefix='dummy')\n",
    "\n",
    "# Some Stackoverflow code to fix duplicate column names\n",
    "cols = pd.Series(data.columns)\n",
    "\n",
    "for dup in data.columns.get_duplicates():\n",
    "    cols[data.columns.get_loc(dup)] =\\\n",
    "    [dup+'.'+str(d_idx) if d_idx!=0 else dup for d_idx in range(data.columns.get_loc(dup).sum())]\n",
    "\n",
    "data.columns = cols\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_in_3    5278\n",
       "numdat_3    5067\n",
       "shar4_3     4271\n",
       "attr7_3     4271\n",
       "sinc7_3     4271\n",
       "intel7_3    4271\n",
       "fun7_3      4271\n",
       "amb7_3      4271\n",
       "attr4_3     4271\n",
       "sinc4_3     4271\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just a sample of how much missing data we have.\n",
    "data.isnull().sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positin1 has 1836 missing values\n",
      "regressing positin1 with an r2 of 0.885\n",
      "\n",
      "int_corr has 148 missing values\n",
      "averaging int_corr\n",
      "\n",
      "age_o has 89 missing values\n",
      "averaging age_o\n",
      "\n",
      "race_o has 58 missing values\n",
      "averaging race_o\n",
      "\n",
      "pf_o_att has 74 missing values\n",
      "regressing pf_o_att with an r2 of 0.261\n",
      "\n",
      "pf_o_sin has 74 missing values\n",
      "averaging pf_o_sin\n",
      "\n",
      "pf_o_int has 74 missing values\n",
      "averaging pf_o_int\n",
      "\n",
      "pf_o_fun has 83 missing values\n",
      "averaging pf_o_fun\n",
      "\n",
      "pf_o_amb has 92 missing values\n",
      "averaging pf_o_amb\n",
      "\n",
      "pf_o_sha has 114 missing values\n",
      "averaging pf_o_sha\n",
      "\n",
      "sinc_o has 232 missing values\n",
      "averaging sinc_o\n",
      "\n",
      "intel_o has 242 missing values\n",
      "averaging intel_o\n",
      "\n",
      "amb_o has 505 missing values\n",
      "averaging amb_o\n",
      "\n",
      "met_o has 314 missing values\n",
      "averaging met_o\n",
      "\n",
      "age has 89 missing values\n",
      "regressing age with an r2 of 0.987\n",
      "\n",
      "field_cd has 77 missing values\n",
      "regressing field_cd with an r2 of 0.999\n",
      "\n",
      "race has 58 missing values\n",
      "regressing race with an r2 of 0.999\n",
      "\n",
      "imprace has 74 missing values\n",
      "regressing imprace with an r2 of 0.993\n",
      "\n",
      "imprelig has 74 missing values\n",
      "regressing imprelig with an r2 of 0.998\n",
      "\n",
      "goal has 74 missing values\n",
      "regressing goal with an r2 of 0.997\n",
      "\n",
      "date has 92 missing values\n",
      "regressing date with an r2 of 0.996\n",
      "\n",
      "go_out has 74 missing values\n",
      "regressing go_out with an r2 of 0.998\n",
      "\n",
      "career_c has 133 missing values\n",
      "regressing career_c with an r2 of 0.995\n",
      "\n",
      "sports has 74 missing values\n",
      "regressing sports with an r2 of 0.995\n",
      "\n",
      "tvsports has 74 missing values\n",
      "regressing tvsports with an r2 of 0.997\n",
      "\n",
      "exercise has 74 missing values\n",
      "regressing exercise with an r2 of 0.996\n",
      "\n",
      "dining has 74 missing values\n",
      "regressing dining with an r2 of 0.986\n",
      "\n",
      "museums has 74 missing values\n",
      "regressing museums with an r2 of 0.987\n",
      "\n",
      "art has 74 missing values\n",
      "regressing art with an r2 of 0.997\n",
      "\n",
      "hiking has 74 missing values\n",
      "regressing hiking with an r2 of 0.998\n",
      "\n",
      "gaming has 74 missing values\n",
      "regressing gaming with an r2 of 0.997\n",
      "\n",
      "clubbing has 74 missing values\n",
      "regressing clubbing with an r2 of 0.999\n",
      "\n",
      "reading has 74 missing values\n",
      "regressing reading with an r2 of 0.991\n",
      "\n",
      "tv has 74 missing values\n",
      "regressing tv with an r2 of 0.996\n",
      "\n",
      "theater has 74 missing values\n",
      "regressing theater with an r2 of 0.99\n",
      "\n",
      "movies has 74 missing values\n",
      "regressing movies with an r2 of 0.973\n",
      "\n",
      "concerts has 74 missing values\n",
      "regressing concerts with an r2 of 0.996\n",
      "\n",
      "music has 74 missing values\n",
      "regressing music with an r2 of 0.988\n",
      "\n",
      "shopping has 74 missing values\n",
      "regressing shopping with an r2 of 0.992\n",
      "\n",
      "yoga has 74 missing values\n",
      "regressing yoga with an r2 of 0.997\n",
      "\n",
      "exphappy has 96 missing values\n",
      "regressing exphappy with an r2 of 0.997\n",
      "\n",
      "expnum has 3971 missing values\n",
      "regressing expnum with an r2 of 1.0\n",
      "\n",
      "attr1_1 has 74 missing values\n",
      "regressing attr1_1 with an r2 of 0.992\n",
      "\n",
      "sinc1_1 has 74 missing values\n",
      "regressing sinc1_1 with an r2 of 0.995\n",
      "\n",
      "intel1_1 has 74 missing values\n",
      "regressing intel1_1 with an r2 of 0.995\n",
      "\n",
      "fun1_1 has 83 missing values\n",
      "regressing fun1_1 with an r2 of 0.996\n",
      "\n",
      "amb1_1 has 92 missing values\n",
      "regressing amb1_1 with an r2 of 0.996\n",
      "\n",
      "shar1_1 has 114 missing values\n",
      "regressing shar1_1 with an r2 of 0.993\n",
      "\n",
      "attr4_1 has 1874 missing values\n",
      "regressing attr4_1 with an r2 of 0.988\n",
      "\n",
      "sinc4_1 has 1874 missing values\n",
      "regressing sinc4_1 with an r2 of 0.987\n",
      "\n",
      "intel4_1 has 1874 missing values\n",
      "regressing intel4_1 with an r2 of 0.988\n",
      "\n",
      "fun4_1 has 1874 missing values\n",
      "regressing fun4_1 with an r2 of 0.999\n",
      "\n",
      "amb4_1 has 1874 missing values\n",
      "regressing amb4_1 with an r2 of 0.995\n",
      "\n",
      "shar4_1 has 1896 missing values\n",
      "regressing shar4_1 with an r2 of 0.997\n",
      "\n",
      "attr2_1 has 74 missing values\n",
      "regressing attr2_1 with an r2 of 0.994\n",
      "\n",
      "sinc2_1 has 74 missing values\n",
      "regressing sinc2_1 with an r2 of 0.998\n",
      "\n",
      "intel2_1 has 74 missing values\n",
      "regressing intel2_1 with an r2 of 0.997\n",
      "\n",
      "fun2_1 has 74 missing values\n",
      "regressing fun2_1 with an r2 of 0.996\n",
      "\n",
      "amb2_1 has 83 missing values\n",
      "regressing amb2_1 with an r2 of 0.992\n",
      "\n",
      "shar2_1 has 83 missing values\n",
      "regressing shar2_1 with an r2 of 0.998\n",
      "\n",
      "attr3_1 has 100 missing values\n",
      "regressing attr3_1 with an r2 of 0.998\n",
      "\n",
      "sinc3_1 has 100 missing values\n",
      "regressing sinc3_1 with an r2 of 0.999\n",
      "\n",
      "fun3_1 has 100 missing values\n",
      "regressing fun3_1 with an r2 of 0.979\n",
      "\n",
      "intel3_1 has 100 missing values\n",
      "regressing intel3_1 with an r2 of 0.996\n",
      "\n",
      "amb3_1 has 100 missing values\n",
      "regressing amb3_1 with an r2 of 0.997\n",
      "\n",
      "attr5_1 has 1900 missing values\n",
      "regressing attr5_1 with an r2 of 0.996\n",
      "\n",
      "sinc5_1 has 1900 missing values\n",
      "regressing sinc5_1 with an r2 of 0.999\n",
      "\n",
      "intel5_1 has 1900 missing values\n",
      "regressing intel5_1 with an r2 of 0.996\n",
      "\n",
      "fun5_1 has 1900 missing values\n",
      "regressing fun5_1 with an r2 of 0.996\n",
      "\n",
      "amb5_1 has 1900 missing values\n",
      "regressing amb5_1 with an r2 of 0.998\n",
      "\n",
      "sinc has 232 missing values\n",
      "regressing sinc with an r2 of 0.274\n",
      "\n",
      "intel has 242 missing values\n",
      "regressing intel with an r2 of 0.232\n",
      "\n",
      "amb has 505 missing values\n",
      "regressing amb with an r2 of 0.223\n",
      "\n",
      "met has 314 missing values\n",
      "regressing met with an r2 of 0.89\n",
      "\n",
      "match_es has 538 missing values\n",
      "regressing match_es with an r2 of 0.995\n",
      "\n",
      "attr1_s has 4134 missing values\n",
      "regressing attr1_s with an r2 of 0.994\n",
      "\n",
      "sinc1_s has 4134 missing values\n",
      "regressing sinc1_s with an r2 of 0.997\n",
      "\n",
      "intel1_s has 4134 missing values\n",
      "regressing intel1_s with an r2 of 0.992\n",
      "\n",
      "fun1_s has 4134 missing values\n",
      "regressing fun1_s with an r2 of 0.993\n",
      "\n",
      "amb1_s has 4134 missing values\n",
      "regressing amb1_s with an r2 of 0.986\n",
      "\n",
      "shar1_s has 4134 missing values\n",
      "regressing shar1_s with an r2 of 0.944\n",
      "\n",
      "attr3_s has 4149 missing values\n",
      "regressing attr3_s with an r2 of 0.993\n",
      "\n",
      "sinc3_s has 4149 missing values\n",
      "regressing sinc3_s with an r2 of 0.956\n",
      "\n",
      "intel3_s has 4149 missing values\n",
      "regressing intel3_s with an r2 of 0.997\n",
      "\n",
      "fun3_s has 4149 missing values\n",
      "regressing fun3_s with an r2 of 0.985\n",
      "\n",
      "amb3_s has 4149 missing values\n",
      "regressing amb3_s with an r2 of 0.984\n",
      "\n",
      "satis_2 has 708 missing values\n",
      "regressing satis_2 with an r2 of 0.997\n",
      "\n",
      "length has 708 missing values\n",
      "regressing length with an r2 of 0.991\n",
      "\n",
      "numdat_2 has 738 missing values\n",
      "regressing numdat_2 with an r2 of 0.997\n",
      "\n",
      "attr7_2 has 3778 missing values\n",
      "regressing attr7_2 with an r2 of 0.993\n",
      "\n",
      "sinc7_2 has 3807 missing values\n",
      "regressing sinc7_2 with an r2 of 0.999\n",
      "\n",
      "intel7_2 has 3778 missing values\n",
      "regressing intel7_2 with an r2 of 0.983\n",
      "\n",
      "fun7_2 has 3778 missing values\n",
      "regressing fun7_2 with an r2 of 0.997\n",
      "\n",
      "amb7_2 has 3807 missing values\n",
      "regressing amb7_2 with an r2 of 0.986\n",
      "\n",
      "shar7_2 has 3788 missing values\n",
      "regressing shar7_2 with an r2 of 0.991\n",
      "\n",
      "attr1_2 has 726 missing values\n",
      "regressing attr1_2 with an r2 of 0.999\n",
      "\n",
      "sinc1_2 has 708 missing values\n",
      "regressing sinc1_2 with an r2 of 0.998\n",
      "\n",
      "intel1_2 has 708 missing values\n",
      "regressing intel1_2 with an r2 of 0.995\n",
      "\n",
      "fun1_2 has 708 missing values\n",
      "regressing fun1_2 with an r2 of 0.998\n",
      "\n",
      "amb1_2 has 708 missing values\n",
      "regressing amb1_2 with an r2 of 0.997\n",
      "\n",
      "shar1_2 has 708 missing values\n",
      "regressing shar1_2 with an r2 of 0.996\n",
      "\n",
      "attr4_2 has 2387 missing values\n",
      "regressing attr4_2 with an r2 of 0.997\n",
      "\n",
      "sinc4_2 has 2387 missing values\n",
      "regressing sinc4_2 with an r2 of 0.999\n",
      "\n",
      "intel4_2 has 2387 missing values\n",
      "regressing intel4_2 with an r2 of 0.997\n",
      "\n",
      "fun4_2 has 2387 missing values\n",
      "regressing fun4_2 with an r2 of 0.985\n",
      "\n",
      "amb4_2 has 2387 missing values\n",
      "regressing amb4_2 with an r2 of 0.989\n",
      "\n",
      "shar4_2 has 2387 missing values\n",
      "regressing shar4_2 with an r2 of 0.998\n",
      "\n",
      "attr2_2 has 2387 missing values\n",
      "regressing attr2_2 with an r2 of 0.998\n",
      "\n",
      "sinc2_2 has 2387 missing values\n",
      "regressing sinc2_2 with an r2 of 0.998\n",
      "\n",
      "intel2_2 has 2387 missing values\n",
      "regressing intel2_2 with an r2 of 0.991\n",
      "\n",
      "fun2_2 has 2387 missing values\n",
      "regressing fun2_2 with an r2 of 0.992\n",
      "\n",
      "amb2_2 has 2387 missing values\n",
      "regressing amb2_2 with an r2 of 0.992\n",
      "\n",
      "shar2_2 has 2387 missing values\n",
      "regressing shar2_2 with an r2 of 0.999\n",
      "\n",
      "attr3_2 has 708 missing values\n",
      "regressing attr3_2 with an r2 of 0.998\n",
      "\n",
      "sinc3_2 has 708 missing values\n",
      "regressing sinc3_2 with an r2 of 0.997\n",
      "\n",
      "intel3_2 has 708 missing values\n",
      "regressing intel3_2 with an r2 of 0.989\n",
      "\n",
      "fun3_2 has 708 missing values\n",
      "regressing fun3_2 with an r2 of 0.997\n",
      "\n",
      "amb3_2 has 708 missing values\n",
      "regressing amb3_2 with an r2 of 0.997\n",
      "\n",
      "attr5_2 has 2387 missing values\n",
      "regressing attr5_2 with an r2 of 0.988\n",
      "\n",
      "sinc5_2 has 2387 missing values\n",
      "regressing sinc5_2 with an r2 of 0.995\n",
      "\n",
      "intel5_2 has 2387 missing values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regressing intel5_2 with an r2 of 0.999\n",
      "\n",
      "fun5_2 has 2387 missing values\n",
      "regressing fun5_2 with an r2 of 0.997\n",
      "\n",
      "amb5_2 has 2387 missing values\n",
      "regressing amb5_2 with an r2 of 0.994\n",
      "\n",
      "date_3 has 3261 missing values\n",
      "regressing date_3 with an r2 of 0.997\n",
      "\n",
      "numdat_3 has 5067 missing values\n",
      "regressing numdat_3 with an r2 of 0.997\n",
      "\n",
      "num_in_3 has 5278 missing values\n",
      "regressing num_in_3 with an r2 of 1.0\n",
      "\n",
      "attr1_3 has 3261 missing values\n",
      "regressing attr1_3 with an r2 of 0.99\n",
      "\n",
      "sinc1_3 has 3261 missing values\n",
      "regressing sinc1_3 with an r2 of 1.0\n",
      "\n",
      "intel1_3 has 3261 missing values\n",
      "regressing intel1_3 with an r2 of 0.998\n",
      "\n",
      "fun1_3 has 3261 missing values\n",
      "regressing fun1_3 with an r2 of 1.0\n",
      "\n",
      "amb1_3 has 3261 missing values\n",
      "regressing amb1_3 with an r2 of 0.994\n",
      "\n",
      "shar1_3 has 3261 missing values\n",
      "regressing shar1_3 with an r2 of 0.999\n",
      "\n",
      "attr7_3 has 4271 missing values\n",
      "regressing attr7_3 with an r2 of 0.995\n",
      "\n",
      "sinc7_3 has 4271 missing values\n",
      "regressing sinc7_3 with an r2 of 0.999\n",
      "\n",
      "intel7_3 has 4271 missing values\n",
      "regressing intel7_3 with an r2 of 0.997\n",
      "\n",
      "fun7_3 has 4271 missing values\n",
      "regressing fun7_3 with an r2 of 0.986\n",
      "\n",
      "amb7_3 has 4271 missing values\n",
      "regressing amb7_3 with an r2 of 0.999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "# Get null columns\n",
    "null_cols = data.isnull().sum()\n",
    "null_cols = list(null_cols[null_cols != 0].index)\n",
    "\n",
    "for i in null_cols:\n",
    "    print(i, 'has', data[i].isnull().sum(), 'missing values')\n",
    "    x = data.fillna(data[i].mean()).drop(['match', 'id'], axis=1)\n",
    "    y = x.pop(i)\n",
    "    \n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=.2)\n",
    "    reg = KNeighborsRegressor()\n",
    "    reg.fit(xtrain, ytrain)      \n",
    "    pred = reg.predict(xtest)\n",
    "    r2 = r2_score(ytest, pred)\n",
    "    \n",
    "    # If we can reasonably predict these values, do so\n",
    "    if r2 > .2:\n",
    "        print('regressing', i, 'with an r2 of', round(r2, 3))\n",
    "        data['predicted'] = reg.predict(data.fillna(data[i].median()).drop([i, 'match', 'id'], axis=1))\n",
    "        data[i] = np.where(data[i].isnull(), data['predicted'], data[i])\n",
    "        del data['predicted']\n",
    "    \n",
    "    # Otherwise, just take the median\n",
    "    else:\n",
    "        print('averaging', i)\n",
    "        data[i] = data[i].fillna(data[i].median())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partner_data(pid, col):\n",
    "    '''Looks up the person's partner and adds their data\n",
    "    as new features. If the partner ID doesn't exist,\n",
    "    returns a -1.'''\n",
    "    try:\n",
    "        partner = data[data['iid'] == pid].head(1)[col].iloc[0]\n",
    "        if partner:\n",
    "            return partner\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income (where income data is available, take the log difference)\n",
    "data['partner_income'] = data['pid'].apply(get_partner_data, col='income')\n",
    "data['income_difference'] = np.where((data.partner_income == -1) |\\\n",
    "                                     (data.income == -1),\n",
    "                                     -1, np.log1p(np.abs(data.income - data.partner_income)))\n",
    "\n",
    "# Age\n",
    "data['age'].fillna(data['age'].median(), inplace=True)\n",
    "data['age_o'].fillna(data['age_o'].median(), inplace=True)\n",
    "data['age_difference'] = data['age'] - data['age_o']\n",
    "\n",
    "# Attractiveness\n",
    "data['partner_attractiveness'] = data['pid'].apply(get_partner_data, col='attr3_1')\n",
    "data['attr_expectations'] = (10*data['attr1_1']/data['attr1_1'].max()) -\\\n",
    "                                     data['partner_attractiveness']\n",
    "data['partner_attractiveness_2'] = data['pid'].apply(get_partner_data, col='attr5_1')\n",
    "data['attr_expectations_2'] = (10*data['attr1_1']/data['attr1_1'].max()) -\\\n",
    "                                       data['partner_attractiveness_2']\n",
    "\n",
    "# Intelligence\n",
    "data['partner_intelligence'] = data['pid'].apply(get_partner_data, col='intel3_1')\n",
    "data['intel_expectations'] = (10*data['intel1_1']/data['intel1_1'].max()) -\\\n",
    "                                     data['partner_intelligence']\n",
    "data['partner_intelligence_2'] = data['pid'].apply(get_partner_data, col='intel5_1')\n",
    "data['intel_expectations_2'] = (10*data['intel1_1']/data['intel1_1'].max()) -\\\n",
    "                                       data['partner_intelligence_2']\n",
    "\n",
    "# Fun\n",
    "data['partner_fun'] = data['pid'].apply(get_partner_data, col='fun3_1')\n",
    "data['fun_expectations'] = (10*data['fun1_1']/data['fun1_1'].max()) -\\\n",
    "                                     data['partner_fun']\n",
    "data['partner_fun_2'] = data['pid'].apply(get_partner_data, col='fun5_1')\n",
    "data['fun_expectations_2'] = (10*data['fun1_1']/data['fun1_1'].max()) -\\\n",
    "                                       data['partner_fun_2']\n",
    "\n",
    "# Ambitious\n",
    "data['partner_ambition'] = data['pid'].apply(get_partner_data, col='amb3_1')\n",
    "data['amb_expectations'] = (10*data['amb1_1']/data['amb1_1'].max()) -\\\n",
    "                                     data['partner_ambition']\n",
    "data['partner_ambition_2'] = data['pid'].apply(get_partner_data, col='amb5_1')\n",
    "data['amb_expectations_2'] = (10*data['amb1_1']/data['amb1_1'].max()) -\\\n",
    "                                       data['partner_ambition_2']\n",
    "\n",
    "# Sincerity\n",
    "data['partner_sincerity'] = data['pid'].apply(get_partner_data, col='sinc3_1')\n",
    "data['sinc_expectations'] = (10*data['sinc1_1']/data['sinc1_1'].max()) -\\\n",
    "                                     data['partner_sincerity']\n",
    "data['partner_sincerity_2'] = data['pid'].apply(get_partner_data, col='sinc5_1')\n",
    "data['sinc_expectations_2'] = (10*data['sinc1_1']/data['sinc1_1'].max()) -\\\n",
    "                                       data['partner_sincerity_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measuring Expectations\n",
    "# Measuring expectations one more way\n",
    "\n",
    "data['attr_above_avg'] = np.where(data['partner_attractiveness'] > data['partner_attractiveness'].mean(),\n",
    "                                  1, 0)\n",
    "data['attr_imp_above_avg']= np.where(data['attr1_1'] > data['attr1_1'].mean(),\n",
    "                                     1, 0)\n",
    "data['attr_match'] = np.where(data['attr_above_avg'] >= data['attr_imp_above_avg'],\n",
    "                             1, 0)\n",
    "\n",
    "data['sinc_above_avg'] = np.where(data['partner_sincerity'] > data['partner_sincerity'].mean(),\n",
    "                                  1, 0)\n",
    "data['sinc_imp_above_avg']= np.where(data['sinc1_1'] > data['sinc1_1'].mean(),\n",
    "                                     1, 0)\n",
    "data['sinc_match'] = np.where(data['sinc_above_avg'] >= data['sinc_imp_above_avg'],\n",
    "                             1, 0)\n",
    "\n",
    "data['int_above_avg'] = np.where(data['partner_intelligence'] > data['partner_intelligence'].mean(),\n",
    "                                  1, 0)\n",
    "data['int_imp_above_avg']= np.where(data['intel1_1'] > data['intel1_1'].mean(),\n",
    "                                     1, 0)\n",
    "data['int_match'] = np.where(data['int_above_avg'] >= data['int_imp_above_avg'],\n",
    "                             1, 0)\n",
    "\n",
    "data['fun_above_avg'] = np.where(data['partner_fun'] > data['partner_fun'].mean(),\n",
    "                                  1, 0)\n",
    "data['fun_imp_above_avg']= np.where(data['fun1_1'] > data['fun1_1'].mean(),\n",
    "                                     1, 0)\n",
    "data['fun_match'] = np.where(data['fun_above_avg'] >= data['fun_imp_above_avg'],\n",
    "                             1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambition (recall we're doing this one differently)\n",
    "\n",
    "# Temporarily replace missing values with the median\n",
    "data['income_temp'] = np.where(data['income'] == -100, data['income'].median(), data['income'])\n",
    "\n",
    "data['income_above_avg']= np.where(data['income_temp'] > data['income_temp'].median(), 1, 0)\n",
    "data['partner_income_above_avg'] = data['pid'].apply(get_partner_data, col='income_above_avg')\n",
    "\n",
    "data['amb_imp_above_avg'] = np.where(data['amb1_1'] > data['amb1_1'].mean(), 1, 0)\n",
    "\n",
    "data['amb_match'] = np.where(data['partner_income_above_avg'] >= data['amb_imp_above_avg'],\n",
    "                             1, 0)\n",
    "del data['income_temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns\n",
    "\n",
    "for i in cols:\n",
    "    if 'from_' in i:\n",
    "        data['partner_loc'] = data['pid'].apply(get_partner_data, col=i)\n",
    "        data['both_' + i] = np.where((data[i] == 1) & (data['partner_loc'] == 1), 1, 0)\n",
    "        del data['partner_loc']\n",
    "        \n",
    "for i in cols:\n",
    "    if 'school_' in i:\n",
    "        data['partner_edu'] = data['pid'].apply(get_partner_data, col=i)\n",
    "        data['both_' + i] = np.where((data[i] == 1) & (data['partner_edu'] == 1), 1, 0)\n",
    "        del data['partner_edu']\n",
    "        \n",
    "for i in cols:\n",
    "    if 'career_' in i:\n",
    "        data['partner_car'] = data['pid'].apply(get_partner_data, col=i)\n",
    "        data['both_' + i] = np.where((data[i] == 1) & (data['partner_car'] == 1), 1, 0)\n",
    "        del data['partner_car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hobbies\n",
    "hobbies = ['go_out', 'sports', 'tvsports', 'exercise', 'dining', 'museums',\n",
    " 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
    " 'movies', 'concerts', 'music', 'shopping', 'yoga']\n",
    "\n",
    "for i in hobbies:\n",
    "    data['partner_' + i] = data['pid'].apply(get_partner_data, col=i)\n",
    "    data[i + '_in_common'] = np.where((data[i] > 7) & (data['partner_' + i] > 7), 1, 0)\n",
    "    del data['partner_' + i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove dummies when one person has 1\n",
    "\n",
    "#****Remove # to delete data after running first time\n",
    "counts =\\\n",
    "data.groupby('iid').mean().sum(axis=0).sort_values()\n",
    "\n",
    "deleted = []\n",
    "\n",
    "for i in counts[counts < 2].index:\n",
    "    if 'dummy_' in i:\n",
    "        deleted.append(i)\n",
    "       # del data[i]\n",
    "\n",
    "# Fix anything that got counted twice\n",
    "for i in data.columns:\n",
    "    if 'dummy_' in i:\n",
    "        data[i] = np.where(data[i] >= 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearly 500 features removed. Not bad!\n",
    "print('Deleted:', len(deleted))\n",
    "print('Data shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing Zipcode Data\n",
    "data['zipcode'] = data['zipcode'].astype(str)\n",
    "data['zip_5'] = (data['zipcode'].str[:5]).astype(float)\n",
    "data['zip_4'] = (data['zipcode'].str[:4]).astype(float)\n",
    "data['zip_3'] = (data['zipcode'].str[:3]).astype(float)\n",
    "data['zip_2'] = (data['zipcode'].str[:2]).astype(float)\n",
    "del data['zipcode']\n",
    "\n",
    "data['partner_zip'] = data['pid'].apply(get_partner_data, col='zipcode')\n",
    "data['partner_zip'] = data['partner_zip'].astype(str)\n",
    "data['partner_zip_5'] = (data['partner_zip'].str[:5]).astype(float)\n",
    "data['partner_zip_4'] = (data['partner_zip'].str[:4]).astype(float)\n",
    "data['partner_zip_3'] = (data['partner_zip'].str[:3]).astype(float)\n",
    "data['partner_zip_2'] = (data['partner_zip'].str[:2]).astype(float)\n",
    "del data['partner_zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking correlations\n",
    "cols = data.columns\n",
    "\n",
    "\n",
    "for i in cols:\n",
    "    for j in cols:\n",
    "        if i > j:\n",
    "            corr = np.corrcoef(data[i], data[j])[0,0]\n",
    "            if corr > .90:\n",
    "                print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "data.to_csv('Speed Dating Data Processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Visualizations\n",
    "sns.lmplot(\"attr1_1\", \"amb1_1\",\n",
    "           data=data, hue='gender', fit_reg=False,\n",
    "           x_jitter=5, y_jitter=5,\n",
    "           palette=['magenta', 'blue'],\n",
    "           scatter_kws={'alpha':.05})\n",
    "plt.title('Importance of attractiveness vs. ambition in men and women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data[data.gender == 1].age,\n",
    "            bw=1.5, shade=True, color='blue', label='men')\n",
    "sns.kdeplot(data[data.gender == 0].age,\n",
    "            bw=1.5, shade=True, color='magenta', label='women')\n",
    "plt.title('Age of daters')\n",
    "plt.xlabel('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data[(data.gender == 1) & (data.income > 0)].income,\n",
    "            bw=15000, shade=True, color='blue', label='men')\n",
    "sns.kdeplot(data[(data.gender == 0) & (data.income > 0)].income,\n",
    "            bw=15000, shade=True, color='magenta', label='women')\n",
    "plt.title('Estimated income of daters')\n",
    "\n",
    "plt.xlim(0,150000)\n",
    "plt.xlabel('Zip code median income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "x = data.drop('match', axis=1)\n",
    "y = data['match']\n",
    "\n",
    "# Fixing a csv import bug\n",
    "for i in x.columns:\n",
    "    if 'Unnamed' in i:\n",
    "        del x[i]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=.2)\n",
    "\n",
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using K-Neighbor Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "params = {'n_neighbors': np.arange(1,9,2)}\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=4, scoring=scorer).fit(xtrain, ytrain)\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "cv = cross_val_score(clf, xtrain, ytrain, cv=4, scoring=scorer)\n",
    "\n",
    "print(clf, '\\n')\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "cv = cross_val_score(clf, xtrain, ytrain, cv=4, scoring=scorer)\n",
    "\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "params = {'penalty': ['l1', 'l2'],\n",
    "         'C': [.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=4, scoring=scorer).fit(xtrain, ytrain)\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "cv = cross_val_score(clf, xtrain, ytrain, cv=4, scoring=scorer)\n",
    "\n",
    "print(clf, '\\n')\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "params = {'max_depth': list(np.arange(2,13)) + [None],\n",
    "         'min_samples_leaf': [1,2,3,4,5,6,7,10]}\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=4, scoring=scorer).fit(xtrain, ytrain)\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "cv = cross_val_score(clf, xtrain, ytrain, cv=4, scoring=scorer)\n",
    "\n",
    "print(clf, '\\n')\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "params = {'max_depth': list(np.arange(3,10)) + [None],\n",
    "         'min_samples_leaf': [1,2,3],\n",
    "         'n_estimators': [10,100]}\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=4, scoring=scorer).fit(xtrain, ytrain)\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "cv = cross_val_score(clf, xtrain, ytrain, cv=4, scoring=scorer)\n",
    "\n",
    "print(clf, '\\n')\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Stackoverflow code to fix duplicate column names AGAIN\n",
    "cols = pd.Series(data.columns)\n",
    "\n",
    "for dup in data.columns.get_duplicates():\n",
    "    cols[data.columns.get_loc(dup)] =\\\n",
    "    [dup+'.'+str(d_idx) if d_idx!=0 else dup for d_idx in range(data.columns.get_loc(dup).sum())]\n",
    "\n",
    "data.columns = cols\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "clf = XGBClassifier()\n",
    "params = {'max_depth': list(np.arange(3,10)) + [100],\n",
    "         'base_score': np.arange(.42,.52,.01),\n",
    "         'n_estimators': [100]}\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=4, scoring=scorer).fit(xtrain, ytrain)\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "cv = cross_val_score(clf, xtrain, ytrain, cv=4, scoring=scorer)\n",
    "\n",
    "print(clf, '\\n')\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier()\n",
    "params = {'max_depth': [9],\n",
    "         'base_score': np.arange(.40,.45,.007),\n",
    "         'n_estimators': [100]}\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=4, scoring=scorer).fit(xtrain, ytrain)\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "cv = cross_val_score(clf, xtrain, ytrain, cv=4, scoring=scorer)\n",
    "\n",
    "print(clf, '\\n')\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality Reduction & Grid Search\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('reduce_dim', SelectPercentile()),\n",
    "    ('classify', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Recall that I've run this notebook multiple times\n",
    "# and did a MUCH more exhaustive search previously.\n",
    "PERC_OPTIONS = np.arange(44,58,2) # It consistently settles on a value around 50-55\n",
    "EST_OPTIONS = [400] # Best setting I found; we'll only use this to save time\n",
    "DEPTH_OPTIONS = [6,7,8,9]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim__percentile': PERC_OPTIONS,\n",
    "        'classify__n_estimators': EST_OPTIONS,\n",
    "        'classify__max_depth': DEPTH_OPTIONS,\n",
    "    },\n",
    "]\n",
    "\n",
    "grid =\\\n",
    "GridSearchCV(pl, cv=3, n_jobs=1, param_grid=param_grid, scoring=scorer)\\\n",
    ".fit(xtrain, ytrain)\n",
    "\n",
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['classify']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['reduce_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cross_val_score(model, xtrain, ytrain, cv=4, scoring=scorer)\n",
    "\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MatchMakingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, estimator, threshold=50):\n",
    "        self.estimator = estimator\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = self.estimator.predict_proba(X)[:,1]\n",
    "        pred = np.where(pred > np.percentile(pred, self.threshold), 1, 0)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MatchMakingClassifier(model)\n",
    "params = {'threshold': [48,48.5,49]}\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=5, scoring=scorer).fit(xtrain, ytrain)\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "cv = cross_val_score(clf, xtrain, ytrain, cv=5, scoring=scorer)\n",
    "\n",
    "print(clf, '\\n')\n",
    "print('Mean score:', cv.mean())\n",
    "print('Std Dev:   ', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(xtest)\n",
    "\n",
    "fbeta_score(ytest, pred, beta=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def sensitivity_analysis(x=x, clf=clf, cols=10, print_scrambled=False):\n",
    "    '''\n",
    "    Originally I tried shuffling and scrambling the\n",
    "    data. This takes a LONG time, so I eventually\n",
    "    settled on simply randoming it.\n",
    "    '''\n",
    "    cv_scores = []\n",
    "    \n",
    "    # Average over 5 times\n",
    "    for i in range(5):\n",
    "        x_sens = x.copy()\n",
    "        scrambled = []\n",
    "        while len(scrambled) < cols:\n",
    "            col_to_scramble = choice(x_sens.columns)\n",
    "            scrambled.append(col_to_scramble)\n",
    "            x_sens[col_to_scramble] = np.random.randn(len(x_sens))\n",
    "        if print_scrambled:\n",
    "            print('Scrambed:', scrambled)\n",
    "        cv_scores.append(cross_val_score(clf, x_sens, y, cv=4, scoring=scorer).mean())\n",
    "        \n",
    "    print('Average cross val score ({} columns scrambled):'.format(cols), np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(cols=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(cols=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(cols=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(cols=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(cols=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(cols=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "cv = cross_val_score(clf, x, y, cv=4, scoring=scorer)\n",
    "\n",
    "print('Mean f-beta: {0:.4f} (Std dev: {1:.4f})'.format(cv.mean(),  cv.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cv.mean() - benchmark_score)/benchmark_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from the sklearn documentation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(x.values, y.values):\n",
    "    probas_ = model.fit(x[train], y[train]).predict_proba(x[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i+1, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Luck', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity's sake, we'll train a new XGBoost classifier\n",
    "xg = XGBClassifier(max_depth=12, n_estimators=400).fit(x,y)\n",
    "xg = grid.best_estimator_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, len(x.columns)//2))\n",
    "\n",
    "importances = pd.Series(xg.feature_importances_,\n",
    "                        index=x.columns).sort_values(ascending=True)\n",
    "\n",
    "importances.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
